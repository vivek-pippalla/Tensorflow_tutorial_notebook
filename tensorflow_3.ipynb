{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras: Layers, Models, and the High-Level API\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What Keras is and how it relates to TensorFlow\n",
    "- How to create neural network layers\n",
    "- Different ways to build models (Sequential, Functional, Subclassing)\n",
    "- How to compile and train models\n",
    "- Model evaluation and prediction\n",
    "- Saving and loading models\n",
    "\n",
    "## What is Keras?\n",
    "\n",
    "**Keras** is TensorFlow's high-level API for building and training neural networks. It makes deep learning:\n",
    "- **Easy**: Simple, intuitive API\n",
    "- **Flexible**: Multiple ways to build models\n",
    "- **Powerful**: Built on top of TensorFlow's computational engine\n",
    "\n",
    "Keras is now fully integrated into TensorFlow as `tf.keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Layers\n",
    "\n",
    "**Layers** are the building blocks of neural networks. Each layer:\n",
    "- Takes input tensors and produces output tensors\n",
    "- Has learnable parameters (weights and biases)\n",
    "- Applies a specific transformation to the input\n",
    "\n",
    "Common layer types:\n",
    "- **Dense**: Fully connected layer\n",
    "- **Conv2D**: 2D convolutional layer\n",
    "- **LSTM**: Long Short-Term Memory layer\n",
    "- **Dropout**: Regularization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring individual layers\n",
    "\n",
    "# 1. Dense (Fully Connected) Layer\n",
    "# This is the most common layer type\n",
    "# Formula: output = activation(input @ weights + bias)\n",
    "\n",
    "dense_layer = layers.Dense(\n",
    "    units=64,           # Number of output neurons\n",
    "    activation='relu',  # Activation function to apply\n",
    "    name='hidden_layer' # Optional name for the layer\n",
    ")\n",
    "\n",
    "print(f\"Dense layer: {dense_layer}\")\n",
    "print(f\"Layer name: {dense_layer.name}\")\n",
    "print()\n",
    "\n",
    "# Create some sample input data\n",
    "sample_input = tf.random.normal((32, 10))  # Batch of 32 samples, each with 10 features\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "# Apply the layer to the input\n",
    "output = dense_layer(sample_input)\n",
    "print(f\"Output shape: {output.shape}\")  # Should be (32, 64)\n",
    "print(f\"Output sample (first 5 values): {output[0, :5]}\")\n",
    "print()\n",
    "\n",
    "# After the layer is used, we can inspect its weights\n",
    "print(f\"Layer weights:\")\n",
    "for i, weight in enumerate(dense_layer.weights):\n",
    "    print(f\"  Weight {i}: shape {weight.shape}, name '{weight.name}'\")\n",
    "\n",
    "# The weights are: [kernel (input->output weights), bias]\n",
    "kernel, bias = dense_layer.weights\n",
    "print(f\"\\nKernel (weights) shape: {kernel.shape}\")  # (10, 64) - connects 10 inputs to 64 outputs\n",
    "print(f\"Bias shape: {bias.shape}\")                   # (64,) - one bias per output neuron\n",
    "print(f\"Total parameters: {10 * 64 + 64} = {dense_layer.count_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different types of layers and their purposes\n",
    "\n",
    "print(\"=== Different Layer Types ===\")\n",
    "print()\n",
    "\n",
    "# 1. Activation layers (apply non-linear functions)\n",
    "relu_layer = layers.ReLU(name='relu_activation')\n",
    "sigmoid_layer = layers.Activation('sigmoid', name='sigmoid_activation')\n",
    "\n",
    "test_input = tf.constant([[-1.0, 0.0, 1.0, 2.0]])\n",
    "print(f\"Input: {test_input.numpy()}\")\n",
    "print(f\"After ReLU: {relu_layer(test_input).numpy()}\")      # Negative values become 0\n",
    "print(f\"After Sigmoid: {sigmoid_layer(test_input).numpy()}\") # All values squashed to (0,1)\n",
    "print()\n",
    "\n",
    "# 2. Dropout layer (for regularization - prevents overfitting)\n",
    "dropout_layer = layers.Dropout(\n",
    "    rate=0.3,  # 30% of neurons will be randomly set to 0 during training\n",
    "    name='dropout_regularization'\n",
    ")\n",
    "\n",
    "# Dropout behaves differently during training vs inference\n",
    "test_input = tf.ones((1, 10))  # All ones for easy visualization\n",
    "print(f\"Input (all ones): {test_input.numpy()}\")\n",
    "print(f\"Dropout (training=True): {dropout_layer(test_input, training=True).numpy()}\")\n",
    "print(f\"Dropout (training=False): {dropout_layer(test_input, training=False).numpy()}\")\n",
    "print(\"Note: During inference (training=False), dropout does nothing\")\n",
    "print()\n",
    "\n",
    "# 3. Normalization layers (help with training stability)\n",
    "batch_norm = layers.BatchNormalization(name='batch_normalization')\n",
    "\n",
    "# BatchNorm normalizes inputs to have mean≈0 and std≈1\n",
    "test_input = tf.random.normal((4, 5), mean=10, stddev=5)  # Mean=10, std=5\n",
    "print(f\"Before BatchNorm - Mean: {tf.reduce_mean(test_input):.3f}, Std: {tf.sqrt(tf.reduce_mean(tf.square(test_input - tf.reduce_mean(test_input)))):.3f}\")\n",
    "\n",
    "normalized = batch_norm(test_input, training=True)\n",
    "print(f\"After BatchNorm - Mean: {tf.reduce_mean(normalized):.3f}, Std: {tf.sqrt(tf.reduce_mean(tf.square(normalized - tf.reduce_mean(normalized)))):.3f}\")\n",
    "print()\n",
    "\n",
    "# 4. Reshape layers (change tensor dimensions)\n",
    "flatten_layer = layers.Flatten(name='flatten_layer')\n",
    "reshape_layer = layers.Reshape(target_shape=(2, 5), name='reshape_layer')\n",
    "\n",
    "test_input = tf.random.normal((3, 2, 5))  # 3D tensor\n",
    "print(f\"Original shape: {test_input.shape}\")\n",
    "\n",
    "flattened = flatten_layer(test_input)\n",
    "print(f\"After Flatten: {flattened.shape}\")  # Flattens all dimensions except batch\n",
    "\n",
    "# Note: Reshape works on the non-batch dimensions\n",
    "# Let's reshape flattened back to (2, 5)\n",
    "reshaped = reshape_layer(flattened)\n",
    "print(f\"After Reshape: {reshaped.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models: The Sequential API\n",
    "\n",
    "The **Sequential API** is the simplest way to build models when you have a linear stack of layers.\n",
    "\n",
    "Use Sequential when:\n",
    "- Each layer has exactly one input and one output\n",
    "- Layers are connected in sequence (linear topology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building models with the Sequential API\n",
    "\n",
    "print(\"=== Sequential Model Building ===\")\n",
    "print()\n",
    "\n",
    "# Method 1: Pass layers as a list to Sequential()\n",
    "model_v1 = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(10,), name='hidden1'),  # First layer needs input_shape\n",
    "    layers.Dropout(0.3, name='dropout1'),                                   # Regularization\n",
    "    layers.Dense(32, activation='relu', name='hidden2'),                    # Second hidden layer\n",
    "    layers.Dropout(0.2, name='dropout2'),                                   # More regularization\n",
    "    layers.Dense(3, activation='softmax', name='output')                    # Output layer (3 classes)\n",
    "], name='sequential_model_v1')\n",
    "\n",
    "print(f\"Model v1: {model_v1.name}\")\n",
    "print(f\"Number of layers: {len(model_v1.layers)}\")\n",
    "print()\n",
    "\n",
    "# Method 2: Create empty Sequential and add layers\n",
    "model_v2 = keras.Sequential(name='sequential_model_v2')\n",
    "model_v2.add(layers.Dense(64, activation='relu', input_shape=(10,)))  # Must specify input_shape for first layer\n",
    "model_v2.add(layers.BatchNormalization())                             # Batch normalization after dense layer\n",
    "model_v2.add(layers.Dropout(0.3))\n",
    "model_v2.add(layers.Dense(32, activation='relu'))\n",
    "model_v2.add(layers.BatchNormalization())\n",
    "model_v2.add(layers.Dropout(0.2))\n",
    "model_v2.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "print(f\"Model v2: {model_v2.name}\")\n",
    "print(f\"Number of layers: {len(model_v2.layers)}\")\n",
    "print()\n",
    "\n",
    "# Inspect model architecture\n",
    "print(\"Model v1 Summary:\")\n",
    "model_v1.summary()\n",
    "print()\n",
    "\n",
    "# Test the model with sample data\n",
    "sample_input = tf.random.normal((5, 10))  # Batch of 5 samples\n",
    "output = model_v1(sample_input, training=False)  # Set training=False for inference\n",
    "\n",
    "print(f\"Sample input shape: {sample_input.shape}\")\n",
    "print(f\"Model output shape: {output.shape}\")\n",
    "print(f\"Output (probabilities): {output[0].numpy()}\")\n",
    "print(f\"Sum of probabilities: {tf.reduce_sum(output[0]).numpy():.6f}\")  # Should be 1.0 due to softmax\n",
    "print(f\"Predicted class: {tf.argmax(output[0]).numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Models: The Functional API\n",
    "\n",
    "The **Functional API** is more flexible than Sequential. It can handle:\n",
    "- Models with multiple inputs or outputs\n",
    "- Models with branches or merges\n",
    "- Models with shared layers\n",
    "- Complex topologies\n",
    "\n",
    "Key concept: You call layers on tensors to create a graph of connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building models with the Functional API\n",
    "\n",
    "print(\"=== Functional API Model Building ===\")\n",
    "print()\n",
    "\n",
    "# Example 1: Simple feedforward network (equivalent to Sequential)\n",
    "print(\"1. Simple feedforward network:\")\n",
    "\n",
    "# Define the input\n",
    "inputs = keras.Input(shape=(10,), name='main_input')  # Input tensor\n",
    "print(f\"Input tensor: {inputs}\")\n",
    "\n",
    "# Build the network by calling layers on tensors\n",
    "x = layers.Dense(64, activation='relu', name='hidden1')(inputs)  # Call layer on input\n",
    "x = layers.Dropout(0.3, name='dropout1')(x)                     # Call dropout on previous output\n",
    "x = layers.Dense(32, activation='relu', name='hidden2')(x)      # Call dense layer on previous output\n",
    "x = layers.Dropout(0.2, name='dropout2')(x)\n",
    "outputs = layers.Dense(3, activation='softmax', name='output')(x)  # Final output\n",
    "\n",
    "# Create the model\n",
    "functional_model = keras.Model(inputs=inputs, outputs=outputs, name='functional_model')\n",
    "\n",
    "print(f\"Functional model created: {functional_model.name}\")\n",
    "functional_model.summary()\n",
    "print()\n",
    "\n",
    "# Example 2: Multi-input model\n",
    "print(\"2. Multi-input model:\")\n",
    "\n",
    "# Define multiple inputs\n",
    "numeric_input = keras.Input(shape=(10,), name='numeric_features')    # Numeric features\n",
    "categorical_input = keras.Input(shape=(5,), name='categorical_features')  # Categorical features\n",
    "\n",
    "# Process each input separately\n",
    "numeric_branch = layers.Dense(32, activation='relu', name='numeric_dense')(numeric_input)\n",
    "numeric_branch = layers.Dropout(0.2)(numeric_branch)\n",
    "\n",
    "categorical_branch = layers.Dense(16, activation='relu', name='categorical_dense')(categorical_input)\n",
    "categorical_branch = layers.Dropout(0.2)(categorical_branch)\n",
    "\n",
    "# Combine the branches\n",
    "combined = layers.Concatenate(name='concatenate_features')([numeric_branch, categorical_branch])\n",
    "\n",
    "# Add final layers\n",
    "x = layers.Dense(32, activation='relu', name='combined_hidden')(combined)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid', name='binary_output')(x)  # Binary classification\n",
    "\n",
    "# Create multi-input model\n",
    "multi_input_model = keras.Model(\n",
    "    inputs=[numeric_input, categorical_input], \n",
    "    outputs=outputs, \n",
    "    name='multi_input_model'\n",
    ")\n",
    "\n",
    "multi_input_model.summary()\n",
    "print()\n",
    "\n",
    "# Test the multi-input model\n",
    "numeric_data = tf.random.normal((3, 10))\n",
    "categorical_data = tf.random.normal((3, 5))\n",
    "\n",
    "predictions = multi_input_model([numeric_data, categorical_data])\n",
    "print(f\"Multi-input model predictions: {predictions.numpy().flatten()}\")\n",
    "print()\n",
    "\n",
    "# Example 3: Multi-output model\n",
    "print(\"3. Multi-output model:\")\n",
    "\n",
    "inputs = keras.Input(shape=(20,), name='shared_input')\n",
    "\n",
    "# Shared hidden layers\n",
    "shared = layers.Dense(64, activation='relu', name='shared_hidden1')(inputs)\n",
    "shared = layers.Dropout(0.3)(shared)\n",
    "shared = layers.Dense(32, activation='relu', name='shared_hidden2')(shared)\n",
    "\n",
    "# Multiple output branches\n",
    "classification_output = layers.Dense(3, activation='softmax', name='classification')(shared)\n",
    "regression_output = layers.Dense(1, activation='linear', name='regression')(shared)\n",
    "binary_output = layers.Dense(1, activation='sigmoid', name='binary_classification')(shared)\n",
    "\n",
    "# Create multi-output model\n",
    "multi_output_model = keras.Model(\n",
    "    inputs=inputs,\n",
    "    outputs=[classification_output, regression_output, binary_output],\n",
    "    name='multi_output_model'\n",
    ")\n",
    "\n",
    "multi_output_model.summary()\n",
    "\n",
    "# Test multi-output model\n",
    "test_input = tf.random.normal((2, 20))\n",
    "class_pred, reg_pred, binary_pred = multi_output_model(test_input)\n",
    "\n",
    "print(f\"Classification predictions: {class_pred.numpy()}\")\n",
    "print(f\"Regression predictions: {reg_pred.numpy().flatten()}\")\n",
    "print(f\"Binary predictions: {binary_pred.numpy().flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Subclassing: Maximum Flexibility\n",
    "\n",
    "**Model subclassing** gives you complete control over the forward pass. You define your model as a Python class that inherits from `keras.Model`.\n",
    "\n",
    "Use subclassing when:\n",
    "- You need custom forward pass logic\n",
    "- You want to implement research models\n",
    "- You need dynamic behavior (e.g., variable number of layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model subclassing for maximum flexibility\n",
    "\n",
    "class CustomModel(keras.Model):\n",
    "    \"\"\"Custom model with residual connections and conditional computation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=3, use_residual=True, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        \n",
    "        # Store configuration\n",
    "        self.num_classes = num_classes\n",
    "        self.use_residual = use_residual\n",
    "        \n",
    "        # Define layers in __init__\n",
    "        self.input_layer = layers.Dense(64, activation='relu', name='input_dense')\n",
    "        self.batch_norm1 = layers.BatchNormalization(name='batch_norm1')\n",
    "        self.dropout1 = layers.Dropout(0.3, name='dropout1')\n",
    "        \n",
    "        # Residual block layers\n",
    "        self.hidden1 = layers.Dense(64, activation='relu', name='hidden1')\n",
    "        self.batch_norm2 = layers.BatchNormalization(name='batch_norm2')\n",
    "        self.dropout2 = layers.Dropout(0.2, name='dropout2')\n",
    "        \n",
    "        self.hidden2 = layers.Dense(64, activation='relu', name='hidden2')\n",
    "        self.batch_norm3 = layers.BatchNormalization(name='batch_norm3')\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = layers.Dense(num_classes, activation='softmax', name='output')\n",
    "        \n",
    "        # Additional layer for demonstration\n",
    "        self.auxiliary_output = layers.Dense(1, activation='sigmoid', name='auxiliary')\n",
    "    \n",
    "    def call(self, inputs, training=None, return_auxiliary=False):\n",
    "        \"\"\"Define the forward pass\"\"\"\n",
    "        \n",
    "        # Initial processing\n",
    "        x = self.input_layer(inputs)\n",
    "        x = self.batch_norm1(x, training=training)\n",
    "        x = self.dropout1(x, training=training)\n",
    "        \n",
    "        # Store for potential residual connection\n",
    "        residual = x\n",
    "        \n",
    "        # First hidden block\n",
    "        x = self.hidden1(x)\n",
    "        x = self.batch_norm2(x, training=training)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        \n",
    "        # Second hidden block\n",
    "        x = self.hidden2(x)\n",
    "        x = self.batch_norm3(x, training=training)\n",
    "        \n",
    "        # Residual connection (add input to output)\n",
    "        if self.use_residual:\n",
    "            x = x + residual  # This helps with gradient flow\n",
    "        \n",
    "        # Main output\n",
    "        main_output = self.output_layer(x)\n",
    "        \n",
    "        # Conditional auxiliary output\n",
    "        if return_auxiliary:\n",
    "            aux_output = self.auxiliary_output(x)\n",
    "            return main_output, aux_output\n",
    "        else:\n",
    "            return main_output\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Return model configuration for serialization\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_classes': self.num_classes,\n",
    "            'use_residual': self.use_residual,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# Create and test the custom model\n",
    "print(\"=== Custom Model (Subclassing) ===\")\n",
    "print()\n",
    "\n",
    "custom_model = CustomModel(num_classes=3, use_residual=True, name='custom_residual_model')\n",
    "\n",
    "# Build the model by calling it with sample input\n",
    "sample_input = tf.random.normal((4, 20))\n",
    "output = custom_model(sample_input)\n",
    "\n",
    "print(f\"Custom model created: {custom_model.name}\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print()\n",
    "\n",
    "# Model summary (only works after the model has been built)\n",
    "custom_model.summary()\n",
    "print()\n",
    "\n",
    "# Test conditional output\n",
    "main_out, aux_out = custom_model(sample_input, return_auxiliary=True)\n",
    "print(f\"Main output shape: {main_out.shape}\")\n",
    "print(f\"Auxiliary output shape: {aux_out.shape}\")\n",
    "print()\n",
    "\n",
    "# Compare models with and without residual connections\n",
    "model_with_residual = CustomModel(use_residual=True, name='with_residual')\n",
    "model_without_residual = CustomModel(use_residual=False, name='without_residual')\n",
    "\n",
    "# Build both models\n",
    "_ = model_with_residual(sample_input)\n",
    "_ = model_without_residual(sample_input)\n",
    "\n",
    "print(f\"Model with residual - Total params: {model_with_residual.count_params()}\")\n",
    "print(f\"Model without residual - Total params: {model_without_residual.count_params()}\")\n",
    "print(\"Parameter counts are the same - residual connections don't add parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Models\n",
    "\n",
    "Before training, models need to be **compiled** with:\n",
    "- **Optimizer**: Algorithm for updating weights (SGD, Adam, etc.)\n",
    "- **Loss function**: What to minimize (categorical_crossentropy, mse, etc.)\n",
    "- **Metrics**: What to track during training (accuracy, precision, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling models for training\n",
    "\n",
    "print(\"=== Model Compilation ===\")\n",
    "print()\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "demo_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # 3-class classification\n",
    "], name='demo_model')\n",
    "\n",
    "# Basic compilation\n",
    "demo_model.compile(\n",
    "    optimizer='adam',                    # Adam optimizer with default settings\n",
    "    loss='categorical_crossentropy',     # For multi-class classification with one-hot labels\n",
    "    metrics=['accuracy']                 # Track accuracy during training\n",
    ")\n",
    "\n",
    "print(\"1. Basic compilation:\")\n",
    "print(f\"Optimizer: {demo_model.optimizer.__class__.__name__}\")\n",
    "print(f\"Loss function: {demo_model.loss}\")\n",
    "print(f\"Metrics: {[m.name for m in demo_model.metrics]}\")\n",
    "print()\n",
    "\n",
    "# Advanced compilation with custom settings\n",
    "advanced_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(20,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "], name='advanced_model')\n",
    "\n",
    "advanced_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(\n",
    "        learning_rate=0.001,    # Custom learning rate\n",
    "        beta_1=0.9,            # Momentum parameter\n",
    "        beta_2=0.999,          # RMSprop parameter\n",
    "        epsilon=1e-07          # Small constant for numerical stability\n",
    "    ),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',                                    # Classification accuracy\n",
    "        keras.metrics.TopKCategoricalAccuracy(k=2),   # Top-2 accuracy\n",
    "        keras.metrics.Precision(),                     # Precision\n",
    "        keras.metrics.Recall()                        # Recall\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"2. Advanced compilation:\")\n",
    "print(f\"Optimizer: {advanced_model.optimizer.__class__.__name__}\")\n",
    "print(f\"Learning rate: {advanced_model.optimizer.learning_rate.numpy()}\")\n",
    "print(f\"Metrics: {[m.name for m in advanced_model.metrics]}\")\n",
    "print()\n",
    "\n",
    "# Different compilation options for different problems\n",
    "print(\"3. Problem-specific compilations:\")\n",
    "\n",
    "# Binary classification model\n",
    "binary_model = keras.Sequential([\n",
    "    layers.Dense(32, activation='relu', input_shape=(10,)),\n",
    "    layers.Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
    "])\n",
    "\n",
    "binary_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',  # Loss for binary classification\n",
    "    metrics=['accuracy', 'precision', 'recall']\n",
    ")\n",
    "print(f\"Binary classification - Loss: {binary_model.loss}\")\n",
    "\n",
    "# Regression model\n",
    "regression_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(15,)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')  # Linear activation for regression\n",
    "])\n",
    "\n",
    "regression_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mse',  # Mean Squared Error for regression\n",
    "    metrics=['mae', 'mse']  # Mean Absolute Error and Mean Squared Error\n",
    ")\n",
    "print(f\"Regression - Loss: {regression_model.loss}\")\n",
    "\n",
    "# Multi-output model compilation\n",
    "print(\"\\n4. Multi-output model compilation:\")\n",
    "\n",
    "# Recall our multi-output model from earlier\n",
    "inputs = keras.Input(shape=(20,))\n",
    "shared = layers.Dense(32, activation='relu')(inputs)\n",
    "classification_out = layers.Dense(3, activation='softmax', name='classification')(shared)\n",
    "regression_out = layers.Dense(1, activation='linear', name='regression')(shared)\n",
    "\n",
    "multi_model = keras.Model(inputs=inputs, outputs=[classification_out, regression_out])\n",
    "\n",
    "multi_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={\n",
    "        'classification': 'categorical_crossentropy',  # Different loss for each output\n",
    "        'regression': 'mse'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'classification': 1.0,  # Weight for classification loss\n",
    "        'regression': 0.5       # Weight for regression loss (less important)\n",
    "    },\n",
    "    metrics={\n",
    "        'classification': ['accuracy'],\n",
    "        'regression': ['mae']\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Multi-output model compiled with different losses and metrics for each output\")\n",
    "print(f\"Loss weights: classification=1.0, regression=0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models\n",
    "\n",
    "Once compiled, models can be trained using the `fit()` method. This handles:\n",
    "- Forward pass (prediction)\n",
    "- Loss computation\n",
    "- Backward pass (gradient computation)\n",
    "- Parameter updates\n",
    "- Metrics tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training models with fit()\n",
    "\n",
    "print(\"=== Model Training ===\")\n",
    "print()\n",
    "\n",
    "# Generate synthetic dataset for demonstration\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "n_features = 20\n",
    "n_classes = 3\n",
    "\n",
    "# Create synthetic data\n",
    "X = np.random.randn(n_samples, n_features).astype(np.float32)\n",
    "# Create labels with some pattern (not completely random)\n",
    "y_numeric = (X[:, 0] + X[:, 1] - X[:, 2] + np.random.randn(n_samples) * 0.1).astype(np.float32)\n",
    "y_categorical = np.array([int(val % n_classes) for val in (y_numeric * 3) % n_classes]).astype(np.int32)\n",
    "\n",
    "# Convert to one-hot encoding for categorical crossentropy\n",
    "y_one_hot = keras.utils.to_categorical(y_categorical, n_classes)\n",
    "\n",
    "print(f\"Dataset created:\")\n",
    "print(f\"  Features shape: {X.shape}\")\n",
    "print(f\"  Labels shape: {y_one_hot.shape}\")\n",
    "print(f\"  Class distribution: {np.bincount(y_categorical)}\")\n",
    "print()\n",
    "\n",
    "# Split into train/validation sets\n",
    "split_idx = int(0.8 * n_samples)\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y_one_hot[:split_idx], y_one_hot[split_idx:]\n",
    "\n",
    "print(f\"Train set: {X_train.shape}, Validation set: {X_val.shape}\")\n",
    "print()\n",
    "\n",
    "# Create and compile model\n",
    "training_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "], name='training_demo_model')\n",
    "\n",
    "training_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model compiled. Starting training...\")\n",
    "print()\n",
    "\n",
    "# Train the model\n",
    "history = training_model.fit(\n",
    "    X_train, y_train,                # Training data\n",
    "    batch_size=32,                   # Number of samples per gradient update\n",
    "    epochs=20,                       # Number of times to iterate over the entire dataset\n",
    "    validation_data=(X_val, y_val),  # Data to evaluate on at the end of each epoch\n",
    "    verbose=1                        # 0=silent, 1=progress bar, 2=one line per epoch\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print()\n",
    "\n",
    "# The history object contains training metrics\n",
    "print(\"Training history keys:\", list(history.history.keys()))\n",
    "print(f\"Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "print(f\"Final training loss: {history.history['loss'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining analysis:\")\n",
    "if history.history['val_accuracy'][-1] > history.history['accuracy'][-1]:\n",
    "    print(\"✓ Model is not overfitting (validation accuracy >= training accuracy)\")\n",
    "else:\n",
    "    gap = history.history['accuracy'][-1] - history.history['val_accuracy'][-1]\n",
    "    if gap > 0.05:\n",
    "        print(f\"⚠ Possible overfitting (accuracy gap: {gap:.4f})\")\n",
    "        print(\"  Consider: more regularization, less model complexity, or more data\")\n",
    "    else:\n",
    "        print(\"✓ Model seems well-balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation and Prediction\n",
    "\n",
    "After training, we need to:\n",
    "- **Evaluate** model performance on test data\n",
    "- Make **predictions** on new data\n",
    "- Analyze **model behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation and prediction\n",
    "\n",
    "print(\"=== Model Evaluation and Prediction ===\")\n",
    "print()\n",
    "\n",
    "# Create a test set (simulate new, unseen data)\n",
    "np.random.seed(123)  # Different seed for test data\n",
    "X_test = np.random.randn(200, n_features).astype(np.float32)\n",
    "y_test_numeric = (X_test[:, 0] + X_test[:, 1] - X_test[:, 2] + np.random.randn(200) * 0.1).astype(np.float32)\n",
    "y_test_categorical = np.array([int(val % n_classes) for val in (y_test_numeric * 3) % n_classes]).astype(np.int32)\n",
    "y_test_one_hot = keras.utils.to_categorical(y_test_categorical, n_classes)\n",
    "\n",
    "print(f\"Test set created: {X_test.shape}\")\n",
    "print()\n",
    "\n",
    "# 1. Model evaluation\n",
    "print(\"1. Model Evaluation:\")\n",
    "test_loss, test_accuracy = training_model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print()\n",
    "\n",
    "# 2. Making predictions\n",
    "print(\"2. Making Predictions:\")\n",
    "\n",
    "# Predict probabilities\n",
    "predictions = training_model.predict(X_test, verbose=0)\n",
    "print(f\"Prediction shape: {predictions.shape}\")  # (n_samples, n_classes)\n",
    "print(f\"First 5 predictions (probabilities):\")\n",
    "for i in range(5):\n",
    "    print(f\"  Sample {i}: {predictions[i]} (sum: {np.sum(predictions[i]):.6f})\")\n",
    "print()\n",
    "\n",
    "# Convert probabilities to class predictions\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "print(f\"First 10 predicted classes: {predicted_classes[:10]}\")\n",
    "print(f\"First 10 true classes: {y_test_categorical[:10]}\")\n",
    "print()\n",
    "\n",
    "# 3. Detailed analysis\n",
    "print(\"3. Detailed Analysis:\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test_categorical, predicted_classes)\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print()\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_categorical, predicted_classes))\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "           xticklabels=[f'Class {i}' for i in range(n_classes)],\n",
    "           yticklabels=[f'Class {i}' for i in range(n_classes)])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# 4. Prediction confidence analysis\n",
    "print(\"4. Prediction Confidence Analysis:\")\n",
    "\n",
    "# Calculate confidence (max probability) for each prediction\n",
    "confidences = np.max(predictions, axis=1)\n",
    "print(f\"Confidence statistics:\")\n",
    "print(f\"  Mean confidence: {np.mean(confidences):.4f}\")\n",
    "print(f\"  Min confidence: {np.min(confidences):.4f}\")\n",
    "print(f\"  Max confidence: {np.max(confidences):.4f}\")\n",
    "print(f\"  Std confidence: {np.std(confidences):.4f}\")\n",
    "\n",
    "# Find high and low confidence predictions\n",
    "high_conf_mask = confidences > 0.9\n",
    "low_conf_mask = confidences < 0.5\n",
    "\n",
    "print(f\"High confidence predictions (>0.9): {np.sum(high_conf_mask)} / {len(confidences)}\")\n",
    "print(f\"Low confidence predictions (<0.5): {np.sum(low_conf_mask)} / {len(confidences)}\")\n",
    "\n",
    "if np.sum(high_conf_mask) > 0:\n",
    "    high_conf_accuracy = np.mean(predicted_classes[high_conf_mask] == y_test_categorical[high_conf_mask])\n",
    "    print(f\"Accuracy on high confidence predictions: {high_conf_accuracy:.4f}\")\n",
    "\n",
    "if np.sum(low_conf_mask) > 0:\n",
    "    low_conf_accuracy = np.mean(predicted_classes[low_conf_mask] == y_test_categorical[low_conf_mask])\n",
    "    print(f\"Accuracy on low confidence predictions: {low_conf_accuracy:.4f}\")\n",
    "\n",
    "# Plot confidence distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(confidences, bins=20, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(np.mean(confidences), color='red', linestyle='--', label=f'Mean: {np.mean(confidences):.3f}')\n",
    "plt.title('Prediction Confidence Distribution')\n",
    "plt.xlabel('Confidence (Max Probability)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "correct_predictions = (predicted_classes == y_test_categorical)\n",
    "plt.scatter(confidences[correct_predictions], [1]*np.sum(correct_predictions), \n",
    "           alpha=0.6, label='Correct', color='green', s=10)\n",
    "plt.scatter(confidences[~correct_predictions], [0]*np.sum(~correct_predictions), \n",
    "           alpha=0.6, label='Incorrect', color='red', s=10)\n",
    "plt.title('Confidence vs Correctness')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Prediction Correctness')\n",
    "plt.yticks([0, 1], ['Incorrect', 'Correct'])\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Model evaluation completed!\")\n",
    "print(\"Key insights:\")\n",
    "print(f\"- The model achieved {test_accuracy:.1%} accuracy on unseen test data\")\n",
    "print(f\"- Average prediction confidence: {np.mean(confidences):.1%}\")\n",
    "print(f\"- Model appears {'well-calibrated' if np.mean(confidences) < 0.8 else 'overconfident'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading Models\n",
    "\n",
    "Once trained, models need to be saved for later use. TensorFlow provides several ways to save models:\n",
    "- **Full model**: Architecture + weights + optimizer state\n",
    "- **Weights only**: Just the learned parameters\n",
    "- **SavedModel format**: For production deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading models\n",
    "\n",
    "print(\"=== Saving and Loading Models ===\")\n",
    "print()\n",
    "\n",
    "# 1. Save the entire model (architecture + weights + optimizer)\n",
    "print(\"1. Saving entire model:\")\n",
    "training_model.save('complete_model.h5')\n",
    "print(\"✓ Complete model saved as 'complete_model.h5'\")\n",
    "\n",
    "# Load the complete model\n",
    "loaded_model = keras.models.load_model('complete_model.h5')\n",
    "print(\"✓ Complete model loaded\")\n",
    "\n",
    "# Verify the loaded model works\n",
    "original_predictions = training_model.predict(X_test[:5], verbose=0)\n",
    "loaded_predictions = loaded_model.predict(X_test[:5], verbose=0)\n",
    "predictions_match = np.allclose(original_predictions, loaded_predictions)\n",
    "print(f\"✓ Predictions match: {predictions_match}\")\n",
    "print()\n",
    "\n",
    "# 2. Save only the weights\n",
    "print(\"2. Saving weights only:\")\n",
    "training_model.save_weights('model_weights.h5')\n",
    "print(\"✓ Weights saved as 'model_weights.h5'\")\n",
    "\n",
    "# To load weights, you need the same model architecture\n",
    "new_model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Must compile before loading weights (if you want to continue training)\n",
    "new_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Load the weights\n",
    "new_model.load_weights('model_weights.h5')\n",
    "print(\"✓ Weights loaded into new model\")\n",
    "\n",
    "# Verify weights were loaded correctly\n",
    "weights_predictions = new_model.predict(X_test[:5], verbose=0)\n",
    "weights_match = np.allclose(original_predictions, weights_predictions)\n",
    "print(f\"✓ Weights predictions match: {weights_match}\")\n",
    "print()\n",
    "\n",
    "# 3. SavedModel format (recommended for production)\n",
    "print(\"3. SavedModel format (production-ready):\")\n",
    "training_model.save('saved_model_directory', save_format='tf')\n",
    "print(\"✓ Model saved in SavedModel format\")\n",
    "\n",
    "# Load SavedModel\n",
    "loaded_savedmodel = keras.models.load_model('saved_model_directory')\n",
    "print(\"✓ SavedModel loaded\")\n",
    "\n",
    "savedmodel_predictions = loaded_savedmodel.predict(X_test[:5], verbose=0)\n",
    "savedmodel_match = np.allclose(original_predictions, savedmodel_predictions)\n",
    "print(f\"✓ SavedModel predictions match: {savedmodel_match}\")\n",
    "print()\n",
    "\n",
    "# 4. Model configuration (architecture only)\n",
    "print(\"4. Saving model configuration:\")\n",
    "\n",
    "# Get model configuration as JSON\n",
    "model_json = training_model.to_json()\n",
    "print(\"✓ Model architecture exported to JSON\")\n",
    "\n",
    "# Save to file\n",
    "with open('model_architecture.json', 'w') as f:\n",
    "    f.write(model_json)\n",
    "print(\"✓ Architecture saved to 'model_architecture.json'\")\n",
    "\n",
    "# Load architecture from JSON\n",
    "with open('model_architecture.json', 'r') as f:\n",
    "    loaded_model_json = f.read()\n",
    "\n",
    "reconstructed_model = keras.models.model_from_json(loaded_model_json)\n",
    "print(\"✓ Model architecture loaded from JSON\")\n",
    "print(f\"✓ Reconstructed model has {len(reconstructed_model.layers)} layers\")\n",
    "print()\n",
    "\n",
    "# 5. Best practices for model saving\n",
    "print(\"5. Best Practices Summary:\")\n",
    "print(\"   • Use .save() for complete models (development/research)\")\n",
    "print(\"   • Use .save_weights() when you want to experiment with different architectures\")\n",
    "print(\"   • Use SavedModel format for production deployment\")\n",
    "print(\"   • Save model architecture separately for documentation\")\n",
    "print(\"   • Always verify loaded models produce the same predictions\")\n",
    "print()\n",
    "\n",
    "# 6. Custom objects and saving\n",
    "print(\"6. Saving models with custom objects:\")\n",
    "\n",
    "# Custom activation function\n",
    "def swish_activation(x):\n",
    "    \"\"\"Swish activation function: x * sigmoid(x)\"\"\"\n",
    "    return x * keras.activations.sigmoid(x)\n",
    "\n",
    "# Model with custom activation\n",
    "custom_model = keras.Sequential([\n",
    "    layers.Dense(32, input_shape=(10,)),\n",
    "    layers.Activation(swish_activation),  # Custom activation\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "custom_model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# When saving models with custom objects, you need to provide them when loading\n",
    "custom_model.save('custom_model.h5')\n",
    "print(\"✓ Model with custom activation saved\")\n",
    "\n",
    "# Load with custom objects\n",
    "try:\n",
    "    loaded_custom = keras.models.load_model(\n",
    "        'custom_model.h5',\n",
    "        custom_objects={'swish_activation': swish_activation}\n",
    "    )\n",
    "    print(\"✓ Model with custom activation loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Loading failed: {e}\")\n",
    "\n",
    "print(\"\\n✓ Model saving and loading tutorial completed!\")\n",
    "\n",
    "# Cleanup files\n",
    "import os\n",
    "try:\n",
    "    os.remove('complete_model.h5')\n",
    "    os.remove('model_weights.h5')\n",
    "    os.remove('model_architecture.json')\n",
    "    os.remove('custom_model.h5')\n",
    "    import shutil\n",
    "    shutil.rmtree('saved_model_directory')\n",
    "    print(\"✓ Temporary files cleaned up\")\n",
    "except:\n",
    "    print(\"Note: Some temporary files may remain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Complete Workflow\n",
    "\n",
    "Let's put everything together in a complete machine learning workflow using a real-world-like dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete workflow example: Customer churn prediction\n",
    "\n",
    "print(\"=== Complete Workflow: Customer Churn Prediction ===\")\n",
    "print()\n",
    "\n",
    "# 1. Generate realistic customer data\n",
    "np.random.seed(42)\n",
    "n_customers = 2000\n",
    "\n",
    "# Customer features\n",
    "customer_data = {\n",
    "    'tenure_months': np.random.exponential(12, n_customers),           # How long they've been customers\n",
    "    'monthly_charges': np.random.normal(65, 20, n_customers),          # Monthly bill\n",
    "    'total_charges': np.random.exponential(800, n_customers),          # Total spent\n",
    "    'contract_length': np.random.choice([1, 12, 24], n_customers),     # Contract length\n",
    "    'payment_method': np.random.choice([0, 1, 2, 3], n_customers),     # Payment method (encoded)\n",
    "    'senior_citizen': np.random.choice([0, 1], n_customers, p=[0.8, 0.2]),  # Senior citizen\n",
    "    'partner': np.random.choice([0, 1], n_customers, p=[0.5, 0.5]),    # Has partner\n",
    "    'dependents': np.random.choice([0, 1], n_customers, p=[0.7, 0.3]), # Has dependents\n",
    "    'phone_service': np.random.choice([0, 1], n_customers, p=[0.1, 0.9]), # Phone service\n",
    "    'internet_service': np.random.choice([0, 1, 2], n_customers, p=[0.2, 0.4, 0.4]), # Internet type\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(customer_data)\n",
    "\n",
    "# Ensure non-negative values\n",
    "df['tenure_months'] = np.maximum(df['tenure_months'], 1)\n",
    "df['monthly_charges'] = np.maximum(df['monthly_charges'], 10)\n",
    "df['total_charges'] = np.maximum(df['total_charges'], 50)\n",
    "\n",
    "# Create churn target with realistic pattern\n",
    "# Churn is more likely for: low tenure, high charges, month-to-month contracts\n",
    "churn_probability = (\n",
    "    0.1 +  # Base churn rate\n",
    "    0.3 * (df['contract_length'] == 1) +        # Month-to-month contracts\n",
    "    0.2 * (df['tenure_months'] < 6) +           # New customers\n",
    "    0.1 * (df['monthly_charges'] > 80) +        # High charges\n",
    "    0.1 * (df['senior_citizen'] == 1) -         # Senior citizens churn more\n",
    "    0.1 * (df['dependents'] == 1)               # Dependents reduce churn\n",
    ")\n",
    "\n",
    "churn_probability = np.clip(churn_probability, 0.05, 0.8)  # Keep realistic bounds\n",
    "df
