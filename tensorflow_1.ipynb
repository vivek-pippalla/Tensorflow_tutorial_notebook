{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Fundamentals: Tensors and Basic Operations\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What TensorFlow is and why it's used\n",
    "- How to create and manipulate tensors\n",
    "- Basic tensor operations and broadcasting\n",
    "- Data types and shapes in TensorFlow\n",
    "- Converting between TensorFlow tensors and NumPy arrays\n",
    "\n",
    "## What is TensorFlow?\n",
    "TensorFlow is an open-source machine learning framework developed by Google. It's designed to make it easy to build and deploy machine learning models. The name \"TensorFlow\" comes from the way the library handles data - as tensors (multi-dimensional arrays) that flow through a computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow - the main library we'll be using throughout this course\n",
    "import tensorflow as tf\n",
    "import numpy as np  # NumPy for numerical operations and comparison\n",
    "\n",
    "# Check TensorFlow version - important for compatibility\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Check if GPU is available - this will speed up training significantly\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Built with CUDA: {tf.test.is_built_with_cuda()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Tensors\n",
    "\n",
    "A **tensor** is a generalization of vectors and matrices to potentially higher dimensions. Think of it as:\n",
    "- **Scalar (0-D tensor)**: A single number\n",
    "- **Vector (1-D tensor)**: An array of numbers  \n",
    "- **Matrix (2-D tensor)**: A 2D array of numbers\n",
    "- **3-D tensor and higher**: Multi-dimensional arrays\n",
    "\n",
    "In deep learning:\n",
    "- Images are typically 3-D tensors (height, width, channels)\n",
    "- Video data is 4-D tensors (time, height, width, channels)\n",
    "- Batch of images is 4-D tensors (batch_size, height, width, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors - different ways to initialize tensor data\n",
    "\n",
    "# 1. Scalar (0-D tensor) - just a single number\n",
    "scalar = tf.constant(7)\n",
    "print(f\"Scalar: {scalar}\")\n",
    "print(f\"Scalar shape: {scalar.shape}\")\n",
    "print(f\"Scalar dimensions: {scalar.ndim}\")\n",
    "print()\n",
    "\n",
    "# 2. Vector (1-D tensor) - a list of numbers\n",
    "vector = tf.constant([10, 10])\n",
    "print(f\"Vector: {vector}\")\n",
    "print(f\"Vector shape: {vector.shape}\")  # shape tells us the dimensions\n",
    "print(f\"Vector dimensions: {vector.ndim}\")  # ndim tells us how many dimensions\n",
    "print()\n",
    "\n",
    "# 3. Matrix (2-D tensor) - a 2D array of numbers\n",
    "matrix = tf.constant([[10, 7],\n",
    "                      [3, 2],\n",
    "                      [8, 9]])\n",
    "print(f\"Matrix: {matrix}\")\n",
    "print(f\"Matrix shape: {matrix.shape}\")  # (3, 2) means 3 rows, 2 columns\n",
    "print(f\"Matrix dimensions: {matrix.ndim}\")\n",
    "print()\n",
    "\n",
    "# 4. 3-D tensor - could represent an RGB image (height, width, channels)\n",
    "tensor_3d = tf.constant([[[1, 2, 3],\n",
    "                          [4, 5, 6]],\n",
    "                         [[7, 8, 9],\n",
    "                          [10, 11, 12]]])\n",
    "print(f\"3-D Tensor: {tensor_3d}\")\n",
    "print(f\"3-D Tensor shape: {tensor_3d.shape}\")  # (2, 2, 3)\n",
    "print(f\"3-D Tensor dimensions: {tensor_3d.ndim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors with Different Methods\n",
    "\n",
    "TensorFlow provides many ways to create tensors. Each method is useful in different scenarios:\n",
    "- `tf.constant()`: Creates immutable tensors with fixed values\n",
    "- `tf.Variable()`: Creates mutable tensors that can be updated (used for model parameters)\n",
    "- `tf.zeros()`, `tf.ones()`: Create tensors filled with zeros or ones\n",
    "- `tf.random.normal()`: Create tensors with random values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different ways to create tensors\n",
    "\n",
    "# 1. tf.constant - creates immutable (unchangeable) tensors\n",
    "constant_tensor = tf.constant([1, 2, 3])\n",
    "print(f\"Constant tensor: {constant_tensor}\")\n",
    "\n",
    "# 2. tf.Variable - creates mutable (changeable) tensors\n",
    "# These are typically used for model parameters (weights, biases)\n",
    "variable_tensor = tf.Variable([1, 2, 3])\n",
    "print(f\"Variable tensor: {variable_tensor}\")\n",
    "print(f\"Variable is trainable: {variable_tensor.trainable}\")  # Can this be updated during training?\n",
    "\n",
    "# 3. Create tensors filled with zeros\n",
    "zeros_tensor = tf.zeros(shape=(3, 4))  # 3 rows, 4 columns, all zeros\n",
    "print(f\"Zeros tensor:\\n{zeros_tensor}\")\n",
    "\n",
    "# 4. Create tensors filled with ones\n",
    "ones_tensor = tf.ones(shape=(2, 3, 4))  # 3D tensor filled with ones\n",
    "print(f\"Ones tensor shape: {ones_tensor.shape}\")\n",
    "print(f\"Ones tensor:\\n{ones_tensor}\")\n",
    "\n",
    "# 5. Create tensors with random values\n",
    "# Random normal distribution (mean=0, std=1)\n",
    "random_tensor = tf.random.normal(shape=(3, 2))\n",
    "print(f\"Random normal tensor:\\n{random_tensor}\")\n",
    "\n",
    "# Random uniform distribution (values between 0 and 1)\n",
    "random_uniform = tf.random.uniform(shape=(2, 3))\n",
    "print(f\"Random uniform tensor:\\n{random_uniform}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Data Types (dtypes)\n",
    "\n",
    "**Data type** determines what kind of numbers a tensor can store and how much memory it uses:\n",
    "- `float32`: 32-bit floating point numbers (most common for deep learning)\n",
    "- `float64`: 64-bit floating point numbers (double precision)\n",
    "- `int32`: 32-bit integers\n",
    "- `bool`: Boolean values (True/False)\n",
    "\n",
    "**Why dtype matters:**\n",
    "- Neural networks typically use `float32` for good balance of precision and memory\n",
    "- Using the wrong dtype can cause errors or poor performance\n",
    "- You can't mix different dtypes in operations without explicit conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with different data types\n",
    "\n",
    "# Default data type is usually float32 for floating point numbers\n",
    "float_tensor = tf.constant([1.7, 2.4, 3.6])\n",
    "print(f\"Float tensor dtype: {float_tensor.dtype}\")\n",
    "\n",
    "# Explicitly specify data type\n",
    "float16_tensor = tf.constant([1.7, 2.4], dtype=tf.float16)  # Uses less memory\n",
    "print(f\"Float16 tensor dtype: {float16_tensor.dtype}\")\n",
    "\n",
    "# Integer tensor\n",
    "int_tensor = tf.constant([1, 2, 3], dtype=tf.int32)\n",
    "print(f\"Integer tensor dtype: {int_tensor.dtype}\")\n",
    "\n",
    "# Boolean tensor\n",
    "bool_tensor = tf.constant([True, False, True], dtype=tf.bool)\n",
    "print(f\"Boolean tensor: {bool_tensor}\")\n",
    "print(f\"Boolean tensor dtype: {bool_tensor.dtype}\")\n",
    "\n",
    "# Converting between data types\n",
    "float_tensor_converted = tf.cast(int_tensor, dtype=tf.float32)\n",
    "print(f\"Converted to float32: {float_tensor_converted}\")\n",
    "print(f\"New dtype: {float_tensor_converted.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Attributes and Information\n",
    "\n",
    "Every tensor has important attributes that tell us about its structure:\n",
    "- **Shape**: The dimensions of the tensor (e.g., (3, 4) for 3 rows, 4 columns)\n",
    "- **Rank/Dimensions**: How many dimensions the tensor has\n",
    "- **Dtype**: The data type of the elements\n",
    "- **Size**: Total number of elements in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring tensor attributes\n",
    "example_tensor = tf.random.normal(shape=(2, 3, 4, 5))\n",
    "\n",
    "print(f\"Tensor: {example_tensor.shape}\")\n",
    "print(f\"Shape: {example_tensor.shape}\")  # Dimensions of the tensor\n",
    "print(f\"Rank (number of dimensions): {example_tensor.ndim}\")\n",
    "print(f\"Data type: {example_tensor.dtype}\")\n",
    "print(f\"Size (total elements): {tf.size(example_tensor)}\")\n",
    "print(f\"Size as Python int: {tf.size(example_tensor).numpy()}\")\n",
    "\n",
    "# Get specific dimension sizes\n",
    "print(f\"First dimension size: {example_tensor.shape[0]}\")\n",
    "print(f\"All dimension sizes: {example_tensor.shape.as_list()}\")\n",
    "\n",
    "# Shape can also be accessed as a tensor\n",
    "shape_tensor = tf.shape(example_tensor)\n",
    "print(f\"Shape as tensor: {shape_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations\n",
    "\n",
    "TensorFlow supports all the basic mathematical operations you'd expect:\n",
    "- **Element-wise operations**: Applied to each element individually\n",
    "- **Reduction operations**: Combine elements (like sum, mean)\n",
    "- **Linear algebra operations**: Matrix multiplication, etc.\n",
    "\n",
    "These operations are **vectorized**, meaning they're applied to entire tensors at once, which is much faster than using Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic tensor operations\n",
    "\n",
    "# Create some sample tensors\n",
    "a = tf.constant([1, 2, 3, 4])\n",
    "b = tf.constant([10, 20, 30, 40])\n",
    "\n",
    "print(f\"Tensor a: {a}\")\n",
    "print(f\"Tensor b: {b}\")\n",
    "print()\n",
    "\n",
    "# Element-wise operations (applied to each element)\n",
    "print(\"Element-wise operations:\")\n",
    "print(f\"a + b = {a + b}\")  # Addition\n",
    "print(f\"a - b = {a - b}\")  # Subtraction  \n",
    "print(f\"a * b = {a * b}\")  # Multiplication\n",
    "print(f\"b / a = {b / a}\")  # Division\n",
    "print(f\"a ** 2 = {a ** 2}\")  # Power\n",
    "print()\n",
    "\n",
    "# You can also use TensorFlow functions explicitly\n",
    "print(\"Using TensorFlow functions:\")\n",
    "print(f\"tf.add(a, b) = {tf.add(a, b)}\")\n",
    "print(f\"tf.multiply(a, b) = {tf.multiply(a, b)}\")\n",
    "print(f\"tf.square(a) = {tf.square(a)}\")\n",
    "print(f\"tf.sqrt(tf.cast(a, tf.float32)) = {tf.sqrt(tf.cast(a, tf.float32))}\")\n",
    "print()\n",
    "\n",
    "# Reduction operations (combine all elements)\n",
    "print(\"Reduction operations:\")\n",
    "print(f\"Sum of a: {tf.reduce_sum(a)}\")\n",
    "print(f\"Mean of a: {tf.reduce_mean(tf.cast(a, tf.float32))}\")\n",
    "print(f\"Max of a: {tf.reduce_max(a)}\")\n",
    "print(f\"Min of a: {tf.reduce_min(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Operations\n",
    "\n",
    "**Matrix multiplication** is fundamental to deep learning - it's how data flows through neural network layers.\n",
    "\n",
    "Important distinction:\n",
    "- `*` or `tf.multiply()`: Element-wise multiplication\n",
    "- `@` or `tf.matmul()`: Matrix multiplication\n",
    "\n",
    "For matrix multiplication, the inner dimensions must match: (a, b) × (b, c) = (a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operations\n",
    "\n",
    "# Create 2D tensors (matrices)\n",
    "matrix_a = tf.constant([[1, 2],\n",
    "                        [3, 4],\n",
    "                        [5, 6]])\n",
    "\n",
    "matrix_b = tf.constant([[7, 8, 9],\n",
    "                        [10, 11, 12]])\n",
    "\n",
    "print(f\"Matrix A shape: {matrix_a.shape}\")  # (3, 2)\n",
    "print(f\"Matrix A:\\n{matrix_a}\")\n",
    "print()\n",
    "print(f\"Matrix B shape: {matrix_b.shape}\")  # (2, 3)\n",
    "print(f\"Matrix B:\\n{matrix_b}\")\n",
    "print()\n",
    "\n",
    "# Matrix multiplication: (3,2) × (2,3) = (3,3)\n",
    "matrix_mult_result = tf.matmul(matrix_a, matrix_b)\n",
    "print(f\"Matrix multiplication result shape: {matrix_mult_result.shape}\")\n",
    "print(f\"A @ B =\\n{matrix_mult_result}\")\n",
    "\n",
    "# Alternative syntax using @ operator\n",
    "matrix_mult_result2 = matrix_a @ matrix_b\n",
    "print(f\"Same result using @ operator:\\n{matrix_mult_result2}\")\n",
    "print()\n",
    "\n",
    "# Element-wise multiplication (different from matrix multiplication!)\n",
    "# For this, shapes must be exactly the same or broadcastable\n",
    "matrix_c = tf.constant([[1, 2],\n",
    "                        [3, 4],\n",
    "                        [5, 6]])\n",
    "\n",
    "matrix_d = tf.constant([[2, 2],\n",
    "                        [3, 3],\n",
    "                        [4, 4]])\n",
    "\n",
    "element_wise_mult = matrix_c * matrix_d\n",
    "print(f\"Element-wise multiplication:\\n{element_wise_mult}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Indexing and Slicing\n",
    "\n",
    "Just like NumPy arrays, you can access specific elements or slices of tensors:\n",
    "- Use square brackets `[]` to index\n",
    "- Use colons `:` for slicing ranges\n",
    "- Negative indices count from the end\n",
    "- Multiple dimensions are separated by commas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor indexing and slicing\n",
    "\n",
    "# Create a sample tensor\n",
    "sample_tensor = tf.constant([[1, 2, 3, 4],\n",
    "                            [5, 6, 7, 8],\n",
    "                            [9, 10, 11, 12]])\n",
    "\n",
    "print(f\"Original tensor:\\n{sample_tensor}\")\n",
    "print(f\"Shape: {sample_tensor.shape}\")  # (3, 4)\n",
    "print()\n",
    "\n",
    "# Indexing specific elements\n",
    "print(\"Indexing specific elements:\")\n",
    "print(f\"Element at [0, 0]: {sample_tensor[0, 0]}\")  # First row, first column\n",
    "print(f\"Element at [1, 2]: {sample_tensor[1, 2]}\")  # Second row, third column\n",
    "print(f\"Element at [-1, -1]: {sample_tensor[-1, -1]}\")  # Last row, last column\n",
    "print()\n",
    "\n",
    "# Slicing rows and columns\n",
    "print(\"Slicing:\")\n",
    "print(f\"First row: {sample_tensor[0, :]}\")  # All columns of first row\n",
    "print(f\"First column: {sample_tensor[:, 0]}\")  # All rows of first column\n",
    "print(f\"Last row: {sample_tensor[-1, :]}\")  # All columns of last row\n",
    "print(f\"Middle columns: {sample_tensor[:, 1:3]}\")  # All rows, columns 1-2\n",
    "print()\n",
    "\n",
    "# More complex slicing\n",
    "print(\"Complex slicing:\")\n",
    "print(f\"Top-left 2x2:\\n{sample_tensor[:2, :2]}\")\n",
    "print(f\"Every other element in first row: {sample_tensor[0, ::2]}\")\n",
    "print(f\"Reverse the tensor:\\n{sample_tensor[::-1, ::-1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "**Broadcasting** allows TensorFlow to perform operations on tensors with different shapes by automatically expanding the smaller tensor to match the larger one.\n",
    "\n",
    "Rules for broadcasting:\n",
    "1. Start from the trailing (rightmost) dimensions\n",
    "2. Dimensions are compatible if they're equal, or one of them is 1\n",
    "3. Missing dimensions are assumed to be 1\n",
    "\n",
    "This is very useful in deep learning for operations like adding bias terms to layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Broadcasting examples\n",
    "\n",
    "# Example 1: Adding a scalar to a tensor\n",
    "tensor_2d = tf.constant([[1, 2, 3],\n",
    "                        [4, 5, 6]])\n",
    "scalar = tf.constant(10)\n",
    "\n",
    "print(f\"2D tensor shape: {tensor_2d.shape}\")  # (2, 3)\n",
    "print(f\"Scalar shape: {scalar.shape}\")  # () - empty shape means scalar\n",
    "print(f\"Broadcasting result:\\n{tensor_2d + scalar}\")\n",
    "print()\n",
    "\n",
    "# Example 2: Adding tensors with compatible shapes\n",
    "tensor_a = tf.constant([[1, 2, 3],\n",
    "                       [4, 5, 6]])  # Shape: (2, 3)\n",
    "\n",
    "tensor_b = tf.constant([10, 20, 30])  # Shape: (3,) - this will broadcast to (1, 3), then (2, 3)\n",
    "\n",
    "print(f\"Tensor A shape: {tensor_a.shape}\")\n",
    "print(f\"Tensor B shape: {tensor_b.shape}\")\n",
    "print(f\"Broadcasting result:\\n{tensor_a + tensor_b}\")\n",
    "print()\n",
    "\n",
    "# Example 3: Broadcasting with different dimensions\n",
    "tensor_c = tf.constant([[[1, 2],\n",
    "                        [3, 4]]])  # Shape: (1, 2, 2)\n",
    "\n",
    "tensor_d = tf.constant([[10],\n",
    "                       [20]])     # Shape: (2, 1)\n",
    "\n",
    "print(f\"Tensor C shape: {tensor_c.shape}\")\n",
    "print(f\"Tensor D shape: {tensor_d.shape}\")\n",
    "print(f\"Broadcasting result shape: {(tensor_c + tensor_d).shape}\")  # Results in (1, 2, 2)\n",
    "print(f\"Broadcasting result:\\n{tensor_c + tensor_d}\")\n",
    "\n",
    "# Example of incompatible shapes (this would cause an error)\n",
    "# tensor_e = tf.constant([[1, 2, 3]])  # Shape: (1, 3)\n",
    "# tensor_f = tf.constant([[1], [2]])   # Shape: (2, 1)\n",
    "# result = tensor_e + tensor_f  # This would work! Results in (2, 3)\n",
    "\n",
    "print(\"\\nBroadcasting makes many operations much more convenient!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Between TensorFlow and NumPy\n",
    "\n",
    "TensorFlow tensors and NumPy arrays are closely related:\n",
    "- Use `.numpy()` method to convert TensorFlow tensor to NumPy array\n",
    "- Use `tf.constant()` or `tf.convert_to_tensor()` to convert NumPy array to TensorFlow tensor\n",
    "- They can often be used interchangeably in TensorFlow operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting between TensorFlow tensors and NumPy arrays\n",
    "\n",
    "# Create a TensorFlow tensor\n",
    "tf_tensor = tf.constant([1, 2, 3, 4, 5])\n",
    "print(f\"TensorFlow tensor: {tf_tensor}\")\n",
    "print(f\"Type: {type(tf_tensor)}\")\n",
    "print()\n",
    "\n",
    "# Convert TensorFlow tensor to NumPy array\n",
    "numpy_array = tf_tensor.numpy()\n",
    "print(f\"NumPy array: {numpy_array}\")\n",
    "print(f\"Type: {type(numpy_array)}\")\n",
    "print()\n",
    "\n",
    "# Create a NumPy array\n",
    "numpy_original = np.array([10, 20, 30, 40, 50])\n",
    "print(f\"Original NumPy array: {numpy_original}\")\n",
    "print(f\"Type: {type(numpy_original)}\")\n",
    "print()\n",
    "\n",
    "# Convert NumPy array to TensorFlow tensor\n",
    "tf_from_numpy = tf.constant(numpy_original)\n",
    "print(f\"TensorFlow from NumPy: {tf_from_numpy}\")\n",
    "print(f\"Type: {type(tf_from_numpy)}\")\n",
    "\n",
    "# Alternative conversion method\n",
    "tf_converted = tf.convert_to_tensor(numpy_original)\n",
    "print(f\"TensorFlow converted: {tf_converted}\")\n",
    "print()\n",
    "\n",
    "# TensorFlow operations work with NumPy arrays too!\n",
    "numpy_array1 = np.array([1, 2, 3])\n",
    "numpy_array2 = np.array([4, 5, 6])\n",
    "\n",
    "# TensorFlow automatically converts NumPy arrays\n",
    "tf_result = tf.add(numpy_array1, numpy_array2)\n",
    "print(f\"TensorFlow operation on NumPy arrays: {tf_result}\")\n",
    "print(f\"Result type: {type(tf_result)}\")\n",
    "\n",
    "# Note: Variables need special handling\n",
    "tf_variable = tf.Variable([1, 2, 3])\n",
    "variable_as_numpy = tf_variable.numpy()\n",
    "print(f\"Variable as NumPy: {variable_as_numpy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Exercise: Image Data Simulation\n",
    "\n",
    "Let's apply what we've learned to a practical example. We'll simulate working with image data, which is very common in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical exercise: Working with image-like data\n",
    "\n",
    "# Simulate a batch of RGB images\n",
    "# Shape: (batch_size, height, width, channels)\n",
    "# Let's say we have 32 images, each 28x28 pixels with 3 color channels (RGB)\n",
    "batch_size = 32\n",
    "height = 28\n",
    "width = 28\n",
    "channels = 3\n",
    "\n",
    "# Create random image data (pixel values between 0 and 255)\n",
    "image_batch = tf.random.uniform(\n",
    "    shape=(batch_size, height, width, channels),\n",
    "    minval=0,\n",
    "    maxval=255,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "print(f\"Image batch shape: {image_batch.shape}\")\n",
    "print(f\"Data type: {image_batch.dtype}\")\n",
    "print(f\"Min value: {tf.reduce_min(image_batch)}\")\n",
    "print(f\"Max value: {tf.reduce_max(image_batch)}\")\n",
    "print()\n",
    "\n",
    "# Common preprocessing: Normalize pixel values to [0, 1] range\n",
    "normalized_images = image_batch / 255.0\n",
    "print(f\"After normalization:\")\n",
    "print(f\"Min value: {tf.reduce_min(normalized_images)}\")\n",
    "print(f\"Max value: {tf.reduce_max(normalized_images)}\")\n",
    "print()\n",
    "\n",
    "# Get a single image from the batch\n",
    "single_image = image_batch[0]  # First image in the batch\n",
    "print(f\"Single image shape: {single_image.shape}\")  # (28, 28, 3)\n",
    "\n",
    "# Get all red channels from all images\n",
    "red_channels = image_batch[:, :, :, 0]  # All images, all pixels, red channel only\n",
    "print(f\"Red channels shape: {red_channels.shape}\")  # (32, 28, 28)\n",
    "\n",
    "# Calculate mean pixel value across the batch\n",
    "mean_pixel_value = tf.reduce_mean(normalized_images)\n",
    "print(f\"Mean pixel value across batch: {mean_pixel_value}\")\n",
    "\n",
    "# Calculate mean for each channel\n",
    "mean_per_channel = tf.reduce_mean(normalized_images, axis=[0, 1, 2])  # Average over batch, height, width\n",
    "print(f\"Mean per channel (R, G, B): {mean_per_channel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **What tensors are**: Multi-dimensional arrays that are the fundamental data structure in TensorFlow\n",
    "2. **Creating tensors**: Using `tf.constant()`, `tf.Variable()`, `tf.zeros()`, `tf.ones()`, and random functions\n",
    "3. **Tensor attributes**: Shape, dtype, rank, and size\n",
    "4. **Basic operations**: Element-wise operations, matrix multiplication, and reductions\n",
    "5. **Indexing and slicing**: Accessing specific elements and ranges of tensors\n",
    "6. **Broadcasting**: How TensorFlow handles operations between different shaped tensors\n",
    "7. **NumPy integration**: Converting between TensorFlow tensors and NumPy arrays\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand tensors and basic operations, you're ready to learn about:\n",
    "- Automatic differentiation (AutoDiff) with GradientTape\n",
    "- Building neural network layers\n",
    "- The Keras API for model building\n",
    "\n",
    "Practice these concepts by experimenting with different tensor shapes and operations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
