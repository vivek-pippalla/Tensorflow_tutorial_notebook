{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation with GradientTape\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you will understand:\n",
    "- What automatic differentiation is and why it's crucial for deep learning\n",
    "- How to use TensorFlow's GradientTape for computing gradients\n",
    "- The difference between gradients of scalars and tensors\n",
    "- How to compute gradients of complex functions\n",
    "- Best practices for working with GradientTape\n",
    "\n",
    "## What is Automatic Differentiation?\n",
    "\n",
    "**Automatic differentiation (AutoDiff)** is the process of computing derivatives automatically. In deep learning:\n",
    "- We need gradients to update model parameters during training\n",
    "- GradientTape \"records\" operations on tensors so it can compute gradients later\n",
    "- This is the foundation of backpropagation in neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic GradientTape Usage\n",
    "\n",
    "**GradientTape** is TensorFlow's tool for automatic differentiation. Think of it as a recorder that \"watches\" operations and can compute gradients.\n",
    "\n",
    "Key concepts:\n",
    "- Use `with tf.GradientTape() as tape:` to start recording\n",
    "- The tape watches `tf.Variable` objects by default\n",
    "- Use `tape.watch()` to watch `tf.constant` tensors\n",
    "- Call `tape.gradient(target, sources)` to compute gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic gradient computation\n",
    "# Let's compute the derivative of f(x) = x^2 at x = 3\n",
    "# We know mathematically that df/dx = 2x, so at x=3, df/dx = 6\n",
    "\n",
    "# Create a variable (GradientTape watches variables by default)\n",
    "x = tf.Variable(3.0, name='x')\n",
    "print(f\"Initial x: {x}\")\n",
    "\n",
    "# Use GradientTape to record operations\n",
    "with tf.GradientTape() as tape:\n",
    "    # Define the function f(x) = x^2\n",
    "    y = x ** 2\n",
    "    print(f\"y = x^2 = {y}\")\n",
    "\n",
    "# Compute the gradient dy/dx\n",
    "gradient = tape.gradient(y, x)\n",
    "print(f\"Gradient dy/dx at x=3: {gradient}\")\n",
    "print(f\"Expected (2*3): 6\")\n",
    "print()\n",
    "\n",
    "# Let's try a more complex function: f(x) = 3x^3 + 2x^2 + x + 1\n",
    "# The derivative is: df/dx = 9x^2 + 4x + 1\n",
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = 3 * x**3 + 2 * x**2 + x + 1\n",
    "    print(f\"f(2) = 3(2)^3 + 2(2)^2 + 2 + 1 = {y}\")\n",
    "\n",
    "gradient = tape.gradient(y, x)\n",
    "print(f\"Gradient at x=2: {gradient}\")\n",
    "print(f\"Expected (9*4 + 4*2 + 1): {9*4 + 4*2 + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Watching Constants and Multiple Variables\n",
    "\n",
    "By default, GradientTape only watches `tf.Variable` objects. To compute gradients with respect to constants, you need to explicitly tell the tape to watch them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watching constants\n",
    "# GradientTape doesn't watch constants by default\n",
    "x = tf.constant(3.0)  # This is a constant, not a variable\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)  # Explicitly tell the tape to watch this constant\n",
    "    y = x ** 2\n",
    "\n",
    "gradient = tape.gradient(y, x)\n",
    "print(f\"Gradient of constant: {gradient}\")\n",
    "print()\n",
    "\n",
    "# Multiple variables\n",
    "# Let's compute gradients of f(x, y) = x^2 + 3xy + y^3\n",
    "x = tf.Variable(2.0, name='x')\n",
    "y = tf.Variable(3.0, name='y')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # f(x,y) = x^2 + 3xy + y^3\n",
    "    z = x**2 + 3*x*y + y**3\n",
    "    print(f\"f(2,3) = 2^2 + 3(2)(3) + 3^3 = {z}\")\n",
    "\n",
    "# Compute gradients with respect to both variables\n",
    "gradients = tape.gradient(z, [x, y])\n",
    "print(f\"Gradient with respect to x: {gradients[0]}\")  # df/dx = 2x + 3y\n",
    "print(f\"Gradient with respect to y: {gradients[1]}\")  # df/dy = 3x + 3y^2\n",
    "print(f\"Expected df/dx at (2,3): {2*2 + 3*3}\")\n",
    "print(f\"Expected df/dy at (2,3): {3*2 + 3*3**2}\")\n",
    "\n",
    "# You can also use a dictionary for cleaner code\n",
    "variables = {'x': x, 'y': y}\n",
    "gradients_dict = tape.gradient(z, variables)\n",
    "print(f\"Gradients as dict: {gradients_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistent GradientTape\n",
    "\n",
    "By default, GradientTape can only be used once. If you need to compute multiple gradients from the same tape, use `persistent=True`.\n",
    "\n",
    "**Important**: Remember to delete persistent tapes to free memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persistent GradientTape allows multiple gradient computations\n",
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    # Define multiple functions\n",
    "    z1 = x**2 + y**2  # First function\n",
    "    z2 = x*y          # Second function\n",
    "    z3 = tf.sin(x) + tf.cos(y)  # Third function with trig functions\n",
    "\n",
    "# Now we can compute gradients for all functions\n",
    "grad_z1 = tape.gradient(z1, [x, y])\n",
    "grad_z2 = tape.gradient(z2, [x, y])\n",
    "grad_z3 = tape.gradient(z3, [x, y])\n",
    "\n",
    "print(f\"Function 1: z1 = x^2 + y^2 = {z1}\")\n",
    "print(f\"Gradients of z1: {grad_z1}\")\n",
    "print(f\"Expected: [2x, 2y] = [{2*2}, {2*3}]\")\n",
    "print()\n",
    "\n",
    "print(f\"Function 2: z2 = xy = {z2}\")\n",
    "print(f\"Gradients of z2: {grad_z2}\")\n",
    "print(f\"Expected: [y, x] = [{y.numpy()}, {x.numpy()}]\")\n",
    "print()\n",
    "\n",
    "print(f\"Function 3: z3 = sin(x) + cos(y) = {z3}\")\n",
    "print(f\"Gradients of z3: {grad_z3}\")\n",
    "print(f\"Expected: [cos(x), -sin(y)] = [{tf.cos(x).numpy():.4f}, {-tf.sin(y).numpy():.4f}]\")\n",
    "\n",
    "# Important: Delete the persistent tape to free memory\n",
    "del tape\n",
    "print(\"\\nPersistent tape deleted to free memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-Order Derivatives\n",
    "\n",
    "You can compute second derivatives (and higher) by nesting GradientTapes. This is useful for advanced optimization techniques and some physics-informed neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing higher-order derivatives\n",
    "# Let's compute the second derivative of f(x) = x^4\n",
    "# f(x) = x^4\n",
    "# f'(x) = 4x^3\n",
    "# f''(x) = 12x^2\n",
    "\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "# Nest GradientTapes for higher-order derivatives\n",
    "with tf.GradientTape() as tape2:  # Outer tape for second derivative\n",
    "    with tf.GradientTape() as tape1:  # Inner tape for first derivative\n",
    "        y = x**4\n",
    "    \n",
    "    # Compute first derivative\n",
    "    first_derivative = tape1.gradient(y, x)\n",
    "    print(f\"Function: y = x^4 = {y}\")\n",
    "    print(f\"First derivative: dy/dx = {first_derivative}\")\n",
    "    print(f\"Expected (4x^3): {4 * 2**3}\")\n",
    "\n",
    "# Compute second derivative\n",
    "second_derivative = tape2.gradient(first_derivative, x)\n",
    "print(f\"Second derivative: d²y/dx² = {second_derivative}\")\n",
    "print(f\"Expected (12x^2): {12 * 2**2}\")\n",
    "print()\n",
    "\n",
    "# Example with a more complex function\n",
    "x = tf.Variable(1.0)\n",
    "\n",
    "with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape() as tape1:\n",
    "        # f(x) = e^x * sin(x)\n",
    "        y = tf.exp(x) * tf.sin(x)\n",
    "    \n",
    "    first_deriv = tape1.gradient(y, x)\n",
    "\n",
    "second_deriv = tape2.gradient(first_deriv, x)\n",
    "\n",
    "print(f\"Complex function: y = e^x * sin(x) = {y:.4f}\")\n",
    "print(f\"First derivative: {first_deriv:.4f}\")\n",
    "print(f\"Second derivative: {second_deriv:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients with Respect to Tensors\n",
    "\n",
    "So far we've computed gradients of scalars. But in deep learning, we often need gradients of scalars with respect to tensor parameters (like weights and biases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients with respect to tensors (like neural network weights)\n",
    "\n",
    "# Simulate a simple linear model: y = W*x + b\n",
    "# where W is a weight matrix and b is a bias vector\n",
    "\n",
    "# Create some sample data\n",
    "x = tf.constant([[1.0, 2.0, 3.0],    # Input features (batch_size=2, features=3)\n",
    "                 [4.0, 5.0, 6.0]])\n",
    "\n",
    "# Create model parameters\n",
    "W = tf.Variable([[0.1, 0.2],         # Weight matrix (features=3, outputs=2)\n",
    "                 [0.3, 0.4],\n",
    "                 [0.5, 0.6]])\n",
    "\n",
    "b = tf.Variable([0.1, 0.2])          # Bias vector (outputs=2)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")     # (2, 3)\n",
    "print(f\"Weight shape: {W.shape}\")    # (3, 2)\n",
    "print(f\"Bias shape: {b.shape}\")      # (2,)\n",
    "print()\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # Forward pass: compute predictions\n",
    "    predictions = tf.matmul(x, W) + b  # Linear transformation\n",
    "    print(f\"Predictions shape: {predictions.shape}\")  # (2, 2)\n",
    "    print(f\"Predictions:\\n{predictions}\")\n",
    "    \n",
    "    # Compute a loss (let's use mean squared error with dummy targets)\n",
    "    targets = tf.constant([[1.0, 0.0],   # Dummy target values\n",
    "                          [0.0, 1.0]])\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(predictions - targets))\n",
    "    print(f\"Loss: {loss}\")\n",
    "\n",
    "# Compute gradients of loss with respect to parameters\n",
    "gradients = tape.gradient(loss, [W, b])\n",
    "grad_W, grad_b = gradients\n",
    "\n",
    "print(f\"\\nGradient of loss w.r.t. weights (dL/dW):\")\n",
    "print(f\"Shape: {grad_W.shape}\")  # Same shape as W: (3, 2)\n",
    "print(grad_W)\n",
    "\n",
    "print(f\"\\nGradient of loss w.r.t. bias (dL/db):\")\n",
    "print(f\"Shape: {grad_b.shape}\")  # Same shape as b: (2,)\n",
    "print(grad_b)\n",
    "\n",
    "# These gradients tell us how to update W and b to reduce the loss!\n",
    "print(\"\\nThese gradients can be used to update the parameters:\")\n",
    "learning_rate = 0.01\n",
    "print(f\"New W = W - lr * grad_W\")\n",
    "print(f\"New b = b - lr * grad_b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Example: Training a Simple Model\n",
    "\n",
    "Let's put it all together and train a simple linear model using gradients. This shows the foundation of how neural networks are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical example: Training a linear model with gradient descent\n",
    "\n",
    "# Generate some synthetic data\n",
    "# True relationship: y = 3x + 2 + noise\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "x_data = np.random.uniform(-1, 1, (n_samples, 1)).astype(np.float32)\n",
    "y_data = 3 * x_data + 2 + 0.1 * np.random.randn(n_samples, 1).astype(np.float32)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "x_train = tf.constant(x_data)\n",
    "y_train = tf.constant(y_data)\n",
    "\n",
    "print(f\"Training data shapes: x={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"True relationship: y = 3x + 2\")\n",
    "print()\n",
    "\n",
    "# Initialize model parameters\n",
    "# Our model: y_pred = w * x + b\n",
    "w = tf.Variable(tf.random.normal([1, 1]), name='weight')  # Start with random weight\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')              # Start with zero bias\n",
    "\n",
    "print(f\"Initial parameters: w={w.numpy().flatten()}, b={b.numpy().flatten()}\")\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "losses = []  # To track training progress\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass: compute predictions\n",
    "        y_pred = tf.matmul(x_train, w) + b\n",
    "        \n",
    "        # Compute loss (mean squared error)\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y_train))\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(loss, [w, b])\n",
    "    grad_w, grad_b = gradients\n",
    "    \n",
    "    # Update parameters (gradient descent)\n",
    "    w.assign_sub(learning_rate * grad_w)  # w = w - lr * grad_w\n",
    "    b.assign_sub(learning_rate * grad_b)  # b = b - lr * grad_b\n",
    "    \n",
    "    # Track progress\n",
    "    losses.append(loss.numpy())\n",
    "    \n",
    "    # Print progress every 20 epochs\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Loss = {loss:.4f}, w = {w.numpy().flatten()[0]:.3f}, b = {b.numpy().flatten()[0]:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal parameters: w={w.numpy().flatten()[0]:.3f}, b={b.numpy().flatten()[0]:.3f}\")\n",
    "print(f\"True parameters: w=3.000, b=2.000\")\n",
    "print(f\"Final loss: {losses[-1]:.6f}\")\n",
    "\n",
    "# Plot the training progress\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot data and learned line\n",
    "plt.scatter(x_data, y_data, alpha=0.5, label='Data')\n",
    "x_line = np.linspace(-1, 1, 100)\n",
    "y_line = w.numpy().flatten()[0] * x_line + b.numpy().flatten()[0]\n",
    "y_true_line = 3 * x_line + 2\n",
    "plt.plot(x_line, y_line, 'r-', label=f'Learned: y = {w.numpy().flatten()[0]:.2f}x + {b.numpy().flatten()[0]:.2f}')\n",
    "plt.plot(x_line, y_true_line, 'g--', label='True: y = 3x + 2')\n",
    "plt.title('Linear Regression Result')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSuccess! The model learned the correct relationship using gradients.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Gotchas and Best Practices\n",
    "\n",
    "Here are some important things to remember when working with GradientTape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common gotchas and best practices\n",
    "\n",
    "print(\"=== Common GradientTape Gotchas ===\")\n",
    "print()\n",
    "\n",
    "# 1. Tape can only be used once (unless persistent=True)\n",
    "print(\"1. Tape consumption:\")\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "\n",
    "# First call works\n",
    "grad1 = tape.gradient(y, x)\n",
    "print(f\"First gradient call: {grad1}\")\n",
    "\n",
    "# Second call returns None (tape is consumed)\n",
    "grad2 = tape.gradient(y, x)\n",
    "print(f\"Second gradient call: {grad2}\")\n",
    "print(\"Solution: Use persistent=True if you need multiple gradient calls\")\n",
    "print()\n",
    "\n",
    "# 2. Operations outside the tape are not recorded\n",
    "print(\"2. Operations must be inside the tape:\")\n",
    "x = tf.Variable(3.0)\n",
    "y = x**2  # This operation is OUTSIDE the tape\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    z = y + 1  # Only this operation is recorded\n",
    "\n",
    "grad = tape.gradient(z, x)\n",
    "print(f\"Gradient when y=x^2 is outside tape: {grad}\")\n",
    "print(\"This is None because the tape didn't see the x^2 operation!\")\n",
    "print()\n",
    "\n",
    "# 3. Correct version - all operations inside tape\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2  # Now inside the tape\n",
    "    z = y + 1\n",
    "\n",
    "grad = tape.gradient(z, x)\n",
    "print(f\"Gradient when all operations inside tape: {grad}\")\n",
    "print()\n",
    "\n",
    "# 4. Gradient of non-differentiable operations\n",
    "print(\"3. Non-differentiable operations:\")\n",
    "x = tf.Variable(2.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # tf.cast changes the data type - this breaks the gradient flow\n",
    "    y = tf.cast(x, tf.int32)\n",
    "    z = tf.cast(y, tf.float32) + 1.0\n",
    "\n",
    "grad = tape.gradient(z, x)\n",
    "print(f\"Gradient through type casting: {grad}\")\n",
    "print(\"Casting to integer and back breaks gradient flow!\")\n",
    "print()\n",
    "\n",
    "# 5. Best practices\n",
    "print(\"=== Best Practices ===\")\n",
    "print(\"1. Only put differentiable operations inside the tape\")\n",
    "print(\"2. Use persistent=True only when necessary (and remember to delete)\")\n",
    "print(\"3. Watch constants explicitly if you need their gradients\")\n",
    "print(\"4. Be careful with operations that break gradient flow (casting, indexing)\")\n",
    "print(\"5. Check that gradients are not None before using them\")\n",
    "\n",
    "# Example of checking gradients\n",
    "x = tf.Variable(1.0)\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**2\n",
    "\n",
    "grad = tape.gradient(y, x)\n",
    "if grad is not None:\n",
    "    print(f\"\\nGradient is valid: {grad}\")\n",
    "    # Safe to use gradient\n",
    "else:\n",
    "    print(\"\\nWarning: Gradient is None!\")\n",
    "    # Handle this case appropriately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: Custom Loss Function with Regularization\n",
    "\n",
    "Let's implement a more complex example that shows how gradients work with custom loss functions and regularization terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced example: Custom loss with L2 regularization\n",
    "\n",
    "# Create synthetic data for a polynomial regression problem\n",
    "np.random.seed(42)\n",
    "n_samples = 50\n",
    "x_data = np.linspace(-2, 2, n_samples).reshape(-1, 1).astype(np.float32)\n",
    "# True function: y = 0.5x^3 - 2x^2 + x + 1 + noise\n",
    "y_data = (0.5 * x_data**3 - 2 * x_data**2 + x_data + 1 + \n",
    "          0.3 * np.random.randn(n_samples, 1)).astype(np.float32)\n",
    "\n",
    "x_train = tf.constant(x_data)\n",
    "y_train = tf.constant(y_data)\n",
    "\n",
    "# Create a polynomial model: y = w3*x^3 + w2*x^2 + w1*x + b\n",
    "w3 = tf.Variable(tf.random.normal([1, 1], stddev=0.1), name='w3')\n",
    "w2 = tf.Variable(tf.random.normal([1, 1], stddev=0.1), name='w2')\n",
    "w1 = tf.Variable(tf.random.normal([1, 1], stddev=0.1), name='w1')\n",
    "b = tf.Variable(tf.zeros([1]), name='bias')\n",
    "\n",
    "parameters = [w3, w2, w1, b]\n",
    "\n",
    "def polynomial_model(x):\n",
    "    \"\"\"Polynomial model: y = w3*x^3 + w2*x^2 + w1*x + b\"\"\"\n",
    "    x2 = tf.square(x)\n",
    "    x3 = tf.multiply(x2, x)\n",
    "    return tf.matmul(x3, w3) + tf.matmul(x2, w2) + tf.matmul(x, w1) + b\n",
    "\n",
    "def custom_loss(y_true, y_pred, parameters, l2_weight=0.01):\n",
    "    \"\"\"Custom loss with L2 regularization\"\"\"\n",
    "    # Mean squared error\n",
    "    mse_loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "    \n",
    "    # L2 regularization (sum of squares of all parameters)\n",
    "    l2_loss = tf.add_n([tf.reduce_sum(tf.square(param)) for param in parameters])\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = mse_loss + l2_weight * l2_loss\n",
    "    \n",
    "    return total_loss, mse_loss, l2_loss\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "l2_weight = 0.001  # Regularization strength\n",
    "\n",
    "# Track training progress\n",
    "total_losses = []\n",
    "mse_losses = []\n",
    "l2_losses = []\n",
    "\n",
    "print(f\"Training polynomial regression with L2 regularization (λ={l2_weight})\")\n",
    "print(f\"Target function: y = 0.5x³ - 2x² + x + 1\")\n",
    "print()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        predictions = polynomial_model(x_train)\n",
    "        \n",
    "        # Compute loss\n",
    "        total_loss, mse_loss, l2_loss = custom_loss(y_train, predictions, parameters, l2_weight)\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradients = tape.gradient(total_loss, parameters)\n",
    "    \n",
    "    # Check if any gradient is None\n",
    "    if any(grad is None for grad in gradients):\n",
    "        print(\"Warning: Some gradients are None!\")\n",
    "        break\n",
    "    \n",
    "    # Update parameters\n",
    "    for param, grad in zip(parameters, gradients):\n",
    "        param.assign_sub(learning_rate * grad)\n",
    "    \n",
    "    # Track progress\n",
    "    total_losses.append(total_loss.numpy())\n",
    "    mse_losses.append(mse_loss.numpy())\n",
    "    l2_losses.append(l2_loss.numpy())\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 50 == 0:\n",
    "        print(f\"Epoch {epoch:3d}: Total Loss = {total_loss:.4f}, \"\n",
    "              f\"MSE = {mse_loss:.4f}, L2 = {l2_loss:.4f}\")\n",
    "        print(f\"  Coefficients: w3={w3.numpy().flatten()[0]:.3f}, \"\n",
    "              f\"w2={w2.numpy().flatten()[0]:.3f}, \"\n",
    "              f\"w1={w1.numpy().flatten()[0]:.3f}, \"\n",
    "              f\"b={b.numpy().flatten()[0]:.3f}\")\n",
    "\n",
    "print(f\"\\nFinal coefficients:\")\n",
    "print(f\"  w3 = {w3.numpy().flatten()[0]:.3f} (target: 0.5)\")\n",
    "print(f\"  w2 = {w2.numpy().flatten()[0]:.3f} (target: -2.0)\")\n",
    "print(f\"  w1 = {w1.numpy().flatten()[0]:.3f} (target: 1.0)\")\n",
    "print(f\"  b  = {b.numpy().flatten()[0]:.3f} (target: 1.0)\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Training curves\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(total_losses, label='Total Loss')\n",
    "plt.plot(mse_losses, label='MSE Loss')\n",
    "plt.plot([l * l2_weight for l in l2_losses], label='L2 Loss (scaled)')\n",
    "plt.title('Training Curves')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 2: Data and fitted curve\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(x_data, y_data, alpha=0.6, label='Data')\n",
    "x_test = np.linspace(-2, 2, 100).reshape(-1, 1)\n",
    "y_pred = polynomial_model(tf.constant(x_test.astype(np.float32))).numpy()\n",
    "y_true = 0.5 * x_test**3 - 2 * x_test**2 + x_test + 1\n",
    "plt.plot(x_test, y_pred, 'r-', linewidth=2, label='Fitted')\n",
    "plt.plot(x_test, y_true, 'g--', linewidth=2, label='True')\n",
    "plt.title('Polynomial Fit')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot 3: Residuals\n",
    "plt.subplot(1, 3, 3)\n",
    "residuals = y_data - polynomial_model(x_train).numpy()\n",
    "plt.scatter(x_data, residuals, alpha=0.6)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.title('Residuals')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Residual')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining complete! The model learned a good approximation of the polynomial.\")\n",
    "print(\"L2 regularization helped prevent overfitting to the noise.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Automatic Differentiation**: TensorFlow can automatically compute gradients of complex functions\n",
    "2. **GradientTape**: The tool for recording operations and computing gradients\n",
    "3. **Basic Usage**: Computing gradients of scalars and tensors\n",
    "4. **Persistent Tapes**: When you need multiple gradient computations\n",
    "5. **Higher-order derivatives**: Computing second derivatives and beyond\n",
    "6. **Practical Applications**: Using gradients for training models\n",
    "7. **Common Pitfalls**: What to watch out for when using GradientTape\n",
    "8. **Advanced Examples**: Custom loss functions with regularization\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Now that you understand tensors and automatic differentiation, you're ready to learn about:\n",
    "- Building neural network layers with Keras\n",
    "- Creating and training complete models\n",
    "- Working with real datasets\n",
    "\n",
    "The gradient computation you learned here is the foundation of all neural network training!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
